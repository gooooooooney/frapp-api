// Shared audio processing and WebSocket utilities
import { optimizedBase64Decode, RingBuffer, createOptimizedWavBlob } from './wav_utils';
import { validateAndConsumeTicket } from '../store/kv';
import { OptimizedAudioStorage, createAudioSession, AudioChunk } from './audio-storage';

// WebSocket message types
export interface AudioStreamStartMessage {
  type: 'audio_stream_start';
}

export interface AudioChunkMessage {
  type: 'audio_chunk';
  data: string;
  vad_state?: 'start' | 'end' | 'cache_asr_trigger' | 'cache_asr_drop';
  vad_offset_ms?: number;
  asr_prompt?: string;
}

export interface AudioStreamEndMessage {
  type: 'audio_stream_end';
}

export type WebSocketMessage = AudioStreamStartMessage | AudioChunkMessage | AudioStreamEndMessage;

// Response message types
export interface TranscriptionResult {
  type: 'transcription_result';
  text: string;
  speechStartTimeMs: number;
  speechEndTimeMs: number;
  timestamp: string;
  is_prefetch?: boolean;
  performance: {
    total_processing_ms: number;
    wav_creation_ms: number;
    api_fetch_ms: number;
    worker_timestamp: string;
    provider: string;
  };
}

export interface TranscriptionError {
  type: 'transcription_error';
  error: string;
  details?: any;
  timestamp: string;
}

// Audio configuration constants
export const CHUNK_DURATION_MS = 128;
export const BYTES_PER_MS = 32; // 16kHz, 16bit, mono = 32000 bytes/sec = 32 bytes/ms

// Global variables for environment
let envVariables: {
  GROQ_API_KEY: string;
  FIREWORKS_API_KEY: string;
  OPENROUTER_API_KEY: string;
  GEMINI_API_KEY: string;
  USE_FIREWORKS: boolean;
  DEBUG_MODE: boolean;
} | null = null;

export function initializeAudioEnv(env: Env): void {
  envVariables = {
    GROQ_API_KEY: env.GROQ_API_KEY,
    FIREWORKS_API_KEY: env.FIREWORKS_API_KEY,
    OPENROUTER_API_KEY: env.OPENROUTER_API_KEY,
    GEMINI_API_KEY: env.GEMINI_API_KEY,
    USE_FIREWORKS: env.USE_FIREWORKS === 'true',
    DEBUG_MODE: env.DEBUG_MODE === 'true',
  };
}

export function getAudioEnv() {
  if (!envVariables) {
    throw new Error('Audio environment not initialized');
  }
  return envVariables;
}

// User context interface for authenticated sessions
export interface AudioSessionContext {
  userId?: string;
  [key: string]: any;
}

// WebSocket session handler
export async function handleAudioSession(websocket: WebSocket, context?: AudioSessionContext): Promise<void> {
  websocket.accept();
  
  const env = getAudioEnv();
  const userId = context?.userId || 'anonymous';
  
  let audioChunkCounter = 0;
  let globalTimeMs = 0;
  let isCaching = false;
  let speechAudioCache: Uint8Array[] = [];
  let previousChunkBuffer = new RingBuffer(256 * BYTES_PER_MS); // 256msç¯å½¢ç¼“å†²åŒº
  let speechStartTimeMs = 0; // è®°å½•è¯­éŸ³å¼€å§‹çš„çœŸå®æ—¶é—´
  
  // é¢„æ„å»ºé™æ€æ¶ˆæ¯æ¨¡æ¿ï¼Œé¿å…é‡å¤JSON.stringify
  const MESSAGE_TEMPLATES = {
    vad_cache_start: '{"type":"vad_cache_start"}',
    vad_cache_end_prefix: '{"type":"vad_cache_end","timestamp":"',
    stream_start_ack_prefix: '{"type":"audio_stream_start_ack","timestamp":"',
    stream_end_ack_prefix: '{"type":"audio_stream_end_ack","receivedChunks":',
  };

  websocket.addEventListener("message", async (event: MessageEvent) => {
    const data = event.data as string;
    {
      // Try to parse as JSON for our custom messages
      try {
        const parsedMessage: WebSocketMessage = JSON.parse(data);
        
        // Handle different message types
        switch (parsedMessage.type) {
          case 'audio_stream_start':
            audioChunkCounter = 0;
            globalTimeMs = 0;
            isCaching = false;
            speechAudioCache = [];
            previousChunkBuffer.clear();
            speechStartTimeMs = 0;
            const startTimestamp = new Date().toISOString();
            console.log(`Audio stream started for user: ${userId}`);
            websocket.send(MESSAGE_TEMPLATES.stream_start_ack_prefix + startTimestamp + '","userId":"' + userId + '"}');
            break;

          // --- ä¸»è¦ä¿®æ”¹éƒ¨åˆ†ï¼šå®ç° VAD ç¼“å­˜é€»è¾‘ ---
          case 'audio_chunk':
            const { vad_state, vad_offset_ms } = parsedMessage; 
            audioChunkCounter++;
            
            // ä¼˜åŒ–çš„éŸ³é¢‘æ•°æ®è§£ç 
            let currentChunk: Uint8Array | null = null;
            if (parsedMessage.data && parsedMessage.data.length > 0) {
                currentChunk = optimizedBase64Decode(parsedMessage.data);
            }
            
            // 1. å¤„ç† VAD 'start' ä¿¡å·
            if (vad_state === 'start') {
                isCaching = true;
                speechAudioCache = [];
                
                // è®°å½•è¯­éŸ³å¼€å§‹çš„çœŸå®æ—¶é—´ï¼ˆè€ƒè™‘offsetï¼‰
                speechStartTimeMs = globalTimeMs + (vad_offset_ms || 0);
                
                // ä»ç¯å½¢ç¼“å†²åŒºè·å–å‰ç¼€æ•°æ®
                if (vad_offset_ms && vad_offset_ms < 0) {
                    const bufferData = previousChunkBuffer.getData();
                    if (bufferData.length > 0) {
                        const offsetBytes = Math.abs(vad_offset_ms) * BYTES_PER_MS;
                        const startByte = Math.max(0, bufferData.length - offsetBytes);
                        const prefixChunk = bufferData.slice(startByte);
                        speechAudioCache.push(prefixChunk);
                    }
                }
                
                websocket.send(MESSAGE_TEMPLATES.vad_cache_start);
            }

            // 2. å¦‚æœæ­£åœ¨ç¼“å­˜ä¸”æœ‰æ•°æ®ï¼Œåˆ™å°†éŸ³é¢‘æ•°æ®å­˜å…¥ cacheï¼ˆVAD endæƒ…å†µç‰¹æ®Šå¤„ç†ï¼‰
            if (isCaching && currentChunk && vad_state !== 'end') {
                speechAudioCache.push(currentChunk);
            }
            
            // ä½¿ç”¨ç¯å½¢ç¼“å†²åŒºç»´æŠ¤256msæ»‘åŠ¨çª—å£
            if (currentChunk) {
                previousChunkBuffer.append(currentChunk);
            }
            globalTimeMs += CHUNK_DURATION_MS;
            
            // 3. å¤„ç† VAD 'end' ä¿¡å·
            if (vad_state === 'end' && isCaching) {
                // è®¡ç®—è¯­éŸ³ç»“æŸçš„çœŸå®æ—¶é—´
                const speechEndTimeMs = globalTimeMs + (vad_offset_ms || 0);
                
                // å¯¹äºVAD endï¼Œéœ€è¦æˆªå–å½“å‰chunkçš„å‰åŠéƒ¨åˆ†ï¼ˆä»å¼€å¤´åˆ°offsetä½ç½®ï¼‰
                if (currentChunk && vad_offset_ms && vad_offset_ms > 0) {
                    const endByte = Math.min(currentChunk.length, vad_offset_ms * BYTES_PER_MS);
                    const endChunk = currentChunk.slice(0, endByte);
                    speechAudioCache.push(endChunk);
                } else if (currentChunk) {
                    speechAudioCache.push(currentChunk);
                }
                
                isCaching = false;

                const cachedData = [...speechAudioCache];
                speechAudioCache = [];

                const vadEndTimestamp = new Date().toISOString();
                websocket.send(MESSAGE_TEMPLATES.vad_cache_end_prefix + vadEndTimestamp + '"}');
                
                // è°ƒç”¨ASR APIï¼Œä¼ é€’æ—¶é—´ä¿¡æ¯
                const asrFunction = env.USE_FIREWORKS ? processAudioWithFireworks : processAudioWithGroq;
                asrFunction(cachedData, websocket, speechStartTimeMs, speechEndTimeMs).catch(err => {
                    websocket.send(JSON.stringify({
                        type: 'transcription_error',
                        error: `Failed to process audio with ${env.USE_FIREWORKS ? 'Fireworks' : 'Groq'} API.`,
                        details: err.message,
                        timestamp: new Date().toISOString()
                    }));
                });
            }
            break;
          // --- ä¿®æ”¹ç»“æŸ ---

          case 'audio_stream_end':
            const endTimestamp = new Date().toISOString();
            websocket.send(MESSAGE_TEMPLATES.stream_end_ack_prefix + audioChunkCounter + ',"timestamp":"' + endTimestamp + '"}');
            break;
          default:
            // Unknown JSON message - send back a specific error message with the unknown message type
            const unknownMessage = parsedMessage as any;
            const unknownType = unknownMessage.type || 'undefined';
            const errorResponse = JSON.stringify({
              error: "Unknown message type received",
              unknownType: unknownType,
              receivedMessage: parsedMessage,
              timestamp: new Date().toISOString()
            });
            websocket.send(errorResponse);
        }
      } catch (parseError) {
        // Not JSON - send back a specific error message with parse details
        const err = parseError as Error;
        const errorResponse = JSON.stringify({
          error: "Failed to parse message as JSON",
          parseError: err.message,
          receivedData: typeof data === 'string' ? data.substring(0, 100) + (data.length > 100 ? '...' : '') : 'Non-string data',
          timestamp: new Date().toISOString()
        });
        websocket.send(errorResponse);
      }
    }
  })

  websocket.addEventListener("close", () => {
    websocket.close();
    console.log(`WebSocket connection closed for user: ${userId}`);
  })
}

export async function processAudioWithFireworks(audioDataChunks: Uint8Array[], websocket: WebSocket, speechStartTimeMs: number, speechEndTimeMs: number, isPrefetch: boolean = false): Promise<void> {
    const env = getAudioEnv();
    const startTime = Date.now();
    
    console.log(`ğŸ”¥ Fireworks ASR called with ${audioDataChunks.length} chunks, speechTime: ${speechStartTimeMs}ms - ${speechEndTimeMs}ms`);
    
    if (!env.FIREWORKS_API_KEY) {
        console.error('âŒ Fireworks ASR failed: FIREWORKS_API_KEY not set');
        throw new Error("FIREWORKS_API_KEY secret is not set in the worker environment.");
    }
    if (audioDataChunks.length === 0) {
        console.error('âŒ Fireworks ASR aborted: empty audioDataChunks array');
        return;
    }

    // 1. ä½¿ç”¨ä¼˜åŒ–çš„WAVåˆ›å»º
    const wavStartTime = Date.now();
    const audioBlob = createOptimizedWavBlob(audioDataChunks);
    const wavEndTime = Date.now();
    
    // å‘é€éŸ³é¢‘æ•°æ®å›å®¢æˆ·ç«¯ä¾›æœ¬åœ°ä¿å­˜æ£€æŸ¥
    if (env.DEBUG_MODE) {
      const audioArrayBuffer = await audioBlob.arrayBuffer();
      const audioBase64 = btoa(String.fromCharCode(...new Uint8Array(audioArrayBuffer)));
      const debugTimestamp = new Date().toISOString();
      // ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥é¿å…JSON.stringify
      websocket.send('{"type":"debug_audio","audioData":"' + audioBase64 + 
                     '","speechStartTimeMs":' + speechStartTimeMs + 
                     ',"speechEndTimeMs":' + speechEndTimeMs + 
                     ',"timestamp":"' + debugTimestamp + '"}');
    }

    // 2. åˆ›å»º FormData
    const formData = new FormData();
    formData.append('file', audioBlob, 'audio.wav');
    formData.append('model', 'whisper-v3-turbo');
    formData.append('temperature', '0');

    // 3. å‘èµ· fetch è¯·æ±‚åˆ° Fireworks API
    const apiStartTime = Date.now();
    const response = await fetch("https://audio-turbo.us-virginia-1.direct.fireworks.ai/v1/audio/transcriptions", {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${env.FIREWORKS_API_KEY}`,
        },
        body: formData,
    });

    // 4. å¤„ç†å“åº”
    const result = await response.json() as any;
    const apiEndTime = Date.now();
    const totalEndTime = Date.now();

    if (!response.ok) {
        console.error("Fireworks API returned an error:", result);
        throw new Error(`Fireworks API error: ${response.status} ${response.statusText} - ${JSON.stringify(result)}`);
    }

    // 5. å¥å£®åœ°æ£€æŸ¥å¹¶æå– text å­—æ®µ
    if (result && result.text !== undefined) {
        const transcriptionResult: TranscriptionResult = {
            type: 'transcription_result',
            text: result.text,
            speechStartTimeMs: speechStartTimeMs,
            speechEndTimeMs: speechEndTimeMs,
            timestamp: new Date().toISOString(),
            is_prefetch: isPrefetch,
            performance: {
                total_processing_ms: totalEndTime - startTime,
                wav_creation_ms: wavEndTime - wavStartTime,
                api_fetch_ms: apiEndTime - apiStartTime,
                worker_timestamp: new Date().toISOString(),
                provider: 'fireworks'
            }
        };
        websocket.send(JSON.stringify(transcriptionResult));
    } else {
        const errorResult: TranscriptionError = {
            type: 'transcription_error',
            error: 'Invalid response format from Fireworks API.',
            details: result,
            timestamp: new Date().toISOString()
        };
        websocket.send(JSON.stringify(errorResult));
    }
}

export async function processAudioWithGroq(audioDataChunks: Uint8Array[], websocket: WebSocket, speechStartTimeMs: number, speechEndTimeMs: number, isPrefetch: boolean = false): Promise<void> {
    const env = getAudioEnv();
    const startTime = Date.now();
    
    console.log(`ğŸ¤– Groq ASR called with ${audioDataChunks.length} chunks, speechTime: ${speechStartTimeMs}ms - ${speechEndTimeMs}ms`);
    
    if (!env.GROQ_API_KEY) {
        console.error('âŒ Groq ASR failed: GROQ_API_KEY not set');
        throw new Error("GROQ_API_KEY secret is not set in the worker environment.");
    }
    if (audioDataChunks.length === 0) {
        console.error('âŒ Groq ASR aborted: empty audioDataChunks array');
        return;
    }

    // 1. ä½¿ç”¨ä¼˜åŒ–çš„WAVåˆ›å»º
    const wavStartTime = Date.now();
    const audioBlob = createOptimizedWavBlob(audioDataChunks);
    const wavEndTime = Date.now();
    
    // å‘é€éŸ³é¢‘æ•°æ®å›å®¢æˆ·ç«¯ä¾›æœ¬åœ°ä¿å­˜æ£€æŸ¥
    if (env.DEBUG_MODE) {
      const audioArrayBuffer = await audioBlob.arrayBuffer();
      const audioBase64 = btoa(String.fromCharCode(...new Uint8Array(audioArrayBuffer)));
      const debugTimestamp = new Date().toISOString();
      // ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥é¿å…JSON.stringify
      websocket.send('{"type":"debug_audio","audioData":"' + audioBase64 + 
                     '","speechStartTimeMs":' + speechStartTimeMs + 
                     ',"speechEndTimeMs":' + speechEndTimeMs + 
                     ',"timestamp":"' + debugTimestamp + '"}');
    }

    // 2. åˆ›å»º FormData
    const formData = new FormData();
    formData.append('file', audioBlob, 'audio.wav');
    formData.append('model', 'whisper-large-v3-turbo');
    formData.append('response_format', 'verbose_json'); 

    // 3. å‘èµ· fetch è¯·æ±‚åˆ° Groq API
    const groqStartTime = Date.now();
    const response = await fetch("https://api.groq.com/openai/v1/audio/transcriptions", {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${env.GROQ_API_KEY}`,
        },
        body: formData,
    });

    // 4. å¤„ç†å“åº”
    const result = await response.json() as any;
    const groqEndTime = Date.now();
    const totalEndTime = Date.now();

    if (!response.ok) {
        // å¦‚æœ HTTP çŠ¶æ€ç ä¸æ˜¯ 2xxï¼Œåˆ™æŠ›å‡ºé”™è¯¯
        console.error("Groq API returned an error:", result);
        throw new Error(`Groq API error: ${response.status} ${response.statusText} - ${JSON.stringify(result)}`);
    }

    // 5. å¥å£®åœ°æ£€æŸ¥å¹¶æå– text å­—æ®µ
    if (result && result.text !== undefined) {
        const transcriptionResult: TranscriptionResult = {
            type: 'transcription_result',
            text: result.text,
            speechStartTimeMs: speechStartTimeMs,
            speechEndTimeMs: speechEndTimeMs,
            timestamp: new Date().toISOString(),
            is_prefetch: isPrefetch,
            performance: {
                total_processing_ms: totalEndTime - startTime,
                wav_creation_ms: wavEndTime - wavStartTime,
                api_fetch_ms: groqEndTime - groqStartTime,
                worker_timestamp: new Date().toISOString(),
                provider: 'groq'
            }
        };
        websocket.send(JSON.stringify(transcriptionResult));
    } else {
        // å¤±è´¥ï¼šè¿”å›çš„ JSON ä¸­æ²¡æœ‰ text å­—æ®µï¼Œå½“ä½œé”™è¯¯å¤„ç†
        const errorResult: TranscriptionError = {
            type: 'transcription_error',
            error: 'Invalid response format from Groq API.',
            details: result,
            timestamp: new Date().toISOString()
        };
        websocket.send(JSON.stringify(errorResult));
    }
}

// Authentication message types for first-message auth
export interface AuthMessage {
  type: 'auth';
  ticket: string;
}

export interface AuthSuccessMessage {
  type: 'auth_success';
  userId: string;
  timestamp: string;
}

export interface AuthErrorMessage {
  type: 'auth_error';
  error: string;
  timestamp: string;
}

// Enhanced WebSocket session handler with First Message Authentication
export async function handleSecureAudioSession(websocket: WebSocket, env: any): Promise<void> {
  console.log('ğŸ” Accepting WebSocket connection...');
  websocket.accept();
  console.log('âœ… WebSocket accepted successfully');
  
  let isAuthenticated = false;
  let userId: string | null = null;
  let audioStorage: OptimizedAudioStorage | null = null;
  const connectionStart = Date.now();

  // Add error and close handlers for debugging
  websocket.addEventListener('error', (event) => {
    console.error('âŒ WebSocket error in handleSecureAudioSession:', event);
  });
  
  websocket.addEventListener('close', (event) => {
    console.log('ğŸ”Œ WebSocket closed in handleSecureAudioSession:', event.code, event.reason);
    if (audioStorage) {
      console.log('ğŸ§¹ Cleaning up audio storage...');
      audioStorage.cleanup().catch(err => console.error('âŒ Audio storage cleanup failed:', err));
    }
  });
  
  websocket.addEventListener('open', () => {
    console.log('âœ… WebSocket opened in handleSecureAudioSession');
  });

  // Authentication timeout (5 seconds)
  const authTimeout = setTimeout(() => {
    if (!isAuthenticated) {
      console.log('âŒ WebSocket connection timed out - no authentication within 5 seconds');
      websocket.send(JSON.stringify({
        type: 'auth_error',
        error: 'Authentication timeout - connection closed',
        timestamp: new Date().toISOString()
      }));
      websocket.close(1008, 'Authentication timeout');
    }
  }, 5000);

  let audioChunkCounter = 0;
  let globalTimeMs = 0;
  let isCaching = false;
  let speechAudioCache: Uint8Array[] = [];
  let previousChunkBuffer = new RingBuffer(256 * BYTES_PER_MS);
  let speechStartTimeMs = 0;
  
  const MESSAGE_TEMPLATES = {
    vad_cache_start: '{"type":"vad_cache_start"}',
    vad_cache_end_prefix: '{"type":"vad_cache_end","timestamp":"',
    stream_start_ack_prefix: '{"type":"audio_stream_start_ack","timestamp":"',
    stream_end_ack_prefix: '{"type":"audio_stream_end_ack","receivedChunks":',
  };

  websocket.addEventListener("message", async (event: MessageEvent) => {
    const data = event.data as string;
    
    try {
      const parsedMessage = JSON.parse(data);
      
      // Handle authentication first
      if (!isAuthenticated) {
        if (parsedMessage.type === 'auth') {
          const authMessage = parsedMessage as AuthMessage;
          
          if (!authMessage.ticket) {
            websocket.send(JSON.stringify({
              type: 'auth_error',
              error: 'Missing ticket in authentication message',
              timestamp: new Date().toISOString()
            }));
            websocket.close(1008, 'Invalid authentication');
            return;
          }

          // Validate ticket
          const validation = await validateAndConsumeTicket(authMessage.ticket, env.WS_TICKETS_KV);
          
          if (!validation) {
            websocket.send(JSON.stringify({
              type: 'auth_error',
              error: 'Invalid or expired ticket',
              timestamp: new Date().toISOString()
            }));
            websocket.close(1008, 'Authentication failed');
            return;
          }

          // Authentication successful
          isAuthenticated = true;
          userId = validation.userId;
          clearTimeout(authTimeout);
          
          // åˆå§‹åŒ–éŸ³é¢‘å­˜å‚¨ (ä¼˜åŒ–çš„å†…å­˜ç¼“å­˜æ–¹æ¡ˆ)
          try {
            audioStorage = createAudioSession(userId, env.FRAPP_FILES_STORE, {
              windowSizeMs: 2 * 60 * 1000,      // 2åˆ†é’Ÿçª—å£
              uploadIntervalMs: 60 * 1000,      // 1åˆ†é’Ÿä¸Šä¼ 
              maxMemoryMB: 10,                  // 10MBé™åˆ¶
              enableDebug: env.DEBUG_MODE === 'true',
              storeOriginalAudio: true,         // å­˜å‚¨å®Œæ•´åŸå§‹éŸ³é¢‘æµ
              storeVadSegments: false           // ä¸å•ç‹¬å­˜å‚¨VADç‰‡æ®µ
            });
            console.log(`ğŸµ Audio storage initialized for user: ${userId}`);
          } catch (error) {
            console.error('âŒ Failed to initialize audio storage:', error);
            // éè‡´å‘½é”™è¯¯ï¼Œç»§ç»­è¿è¡Œä½†ä¸å­˜å‚¨éŸ³é¢‘
          }
          
          console.log(`WebSocket authenticated for user: ${userId} (took ${Date.now() - connectionStart}ms)`);
          
          websocket.send(JSON.stringify({
            type: 'auth_success',
            userId: userId,
            timestamp: new Date().toISOString()
          }));
          
          return;
        } else {
          // Not authenticated and not an auth message
          websocket.send(JSON.stringify({
            type: 'auth_error',
            error: 'Must authenticate first with auth message',
            timestamp: new Date().toISOString()
          }));
          websocket.close(1008, 'Authentication required');
          return;
        }
      }

      // Handle audio messages (only after authentication)
      await handleAudioMessage(parsedMessage, websocket, {
        audioChunkCounter,
        globalTimeMs,
        isCaching,
        speechAudioCache,
        previousChunkBuffer,
        speechStartTimeMs,
        MESSAGE_TEMPLATES,
        userId: userId!,
        audioStorage
      });

    } catch (parseError) {
      const err = parseError as Error;
      const errorResponse = JSON.stringify({
        error: "Failed to parse message as JSON",
        parseError: err.message,
        receivedData: typeof data === 'string' ? data.substring(0, 100) + (data.length > 100 ? '...' : '') : 'Non-string data',
        timestamp: new Date().toISOString()
      });
      websocket.send(errorResponse);
    }
  });

  websocket.addEventListener("close", async () => {
    clearTimeout(authTimeout);
    
    // æ¸…ç†éŸ³é¢‘å­˜å‚¨èµ„æº
    if (audioStorage) {
      try {
        await audioStorage.cleanup();
        console.log(`ğŸ§¹ Audio storage cleaned up for user: ${userId}`);
      } catch (error) {
        console.error('âŒ Error during audio storage cleanup:', error);
      }
    }
    
    const status = isAuthenticated ? `authenticated user: ${userId}` : 'unauthenticated connection';
    console.log(`WebSocket connection closed for ${status}`);
  });
}

// Extracted audio message handler for cleaner code
async function handleAudioMessage(
  parsedMessage: any,
  websocket: WebSocket,
  context: {
    audioChunkCounter: number;
    globalTimeMs: number;
    isCaching: boolean;
    speechAudioCache: Uint8Array[];
    previousChunkBuffer: RingBuffer;
    speechStartTimeMs: number;
    MESSAGE_TEMPLATES: any;
    userId: string;
    audioStorage: OptimizedAudioStorage | null;
  }
) {
  const env = getAudioEnv();

  switch (parsedMessage.type) {
    case 'audio_stream_start':
      context.audioChunkCounter = 0;
      context.globalTimeMs = 0;
      context.isCaching = false;
      context.speechAudioCache = [];
      context.previousChunkBuffer.clear();
      context.speechStartTimeMs = 0;
      const startTimestamp = new Date().toISOString();
      console.log(`Audio stream started for user: ${context.userId}`);
      websocket.send(context.MESSAGE_TEMPLATES.stream_start_ack_prefix + startTimestamp + '","userId":"' + context.userId + '"}');
      break;

    case 'audio_chunk':
      const { vad_state, vad_offset_ms } = parsedMessage;
      context.audioChunkCounter++;
      
      let currentChunk: Uint8Array | null = null;
      if (parsedMessage.data && parsedMessage.data.length > 0) {
        try {
          currentChunk = optimizedBase64Decode(parsedMessage.data);
          console.log(`ğŸµ Decoded audio chunk: ${parsedMessage.data.length} base64 chars -> ${currentChunk?.length || 0} bytes (VAD: ${vad_state || 'none'})`);
        } catch (decodeError) {
          console.error('âŒ Base64 decode failed:', decodeError);
          currentChunk = null;
        }
        
        // åŸå§‹å®Œæ•´éŸ³é¢‘å­˜å‚¨å¤„ç† (å¼‚æ­¥ï¼Œä¸é˜»å¡å®æ—¶è½¬å½•)
        // å­˜å‚¨æ‰€æœ‰éŸ³é¢‘å—ï¼Œä¸ä¾èµ–VADçŠ¶æ€ï¼Œä¿æŒå®Œæ•´éŸ³é¢‘æµè¿ç»­æ€§
        if (currentChunk && context.audioStorage) {
          const audioChunk: AudioChunk = {
            timestamp: context.globalTimeMs,
            data: currentChunk,
            vadState: vad_state,
            vadOffset: vad_offset_ms
          };
          
          // å¼‚æ­¥å¤„ç†ï¼Œä¸ç­‰å¾…å®Œæˆ
          context.audioStorage.processAudioChunk(audioChunk).catch(err => {
            console.error('âŒ Original audio storage processing error:', err);
          });
        }
      } else {
        console.log(`ğŸµ No audio data in message (data length: ${parsedMessage.data?.length || 0})`);
      }
      
      if (vad_state === 'start') {
        console.log(`ğŸµ VAD START event received - globalTimeMs: ${context.globalTimeMs}, vad_offset_ms: ${vad_offset_ms}`);
        
        context.isCaching = true;
        context.speechAudioCache = [];
        context.speechStartTimeMs = context.globalTimeMs + (vad_offset_ms || 0);
        
        // å¤„ç†è´Ÿåç§»ï¼ˆéœ€è¦ä»ring bufferè·å–å‰ç¼€æ•°æ®ï¼‰
        if (vad_offset_ms && vad_offset_ms < 0) {
          const bufferData = context.previousChunkBuffer.getData();
          console.log(`ğŸµ VAD START negative offset: ${vad_offset_ms}ms, ringBufferSize: ${bufferData.length} bytes`);
          
          if (bufferData.length > 0) {
            const offsetBytes = Math.abs(vad_offset_ms) * BYTES_PER_MS;
            const startByte = Math.max(0, bufferData.length - offsetBytes);
            const prefixChunk = bufferData.slice(startByte);
            context.speechAudioCache.push(prefixChunk);
            console.log(`ğŸµ Added prefix chunk: ${prefixChunk.length} bytes (startByte: ${startByte})`);
          }
        }
        
        console.log(`ğŸµ VAD START complete - speechStartTimeMs: ${context.speechStartTimeMs}, cacheLength: ${context.speechAudioCache.length}`);
        websocket.send(context.MESSAGE_TEMPLATES.vad_cache_start);
      }

      // ç¼“å­˜ä¸­é—´éŸ³é¢‘å—ï¼ˆVAD startåˆ°endä¹‹é—´ï¼‰
      if (context.isCaching && currentChunk && vad_state !== 'end') {
        context.speechAudioCache.push(currentChunk);
        console.log(`ğŸµ Cached audio chunk: ${currentChunk.length} bytes (total cache: ${context.speechAudioCache.length} chunks)`);
      } else if (context.isCaching && !currentChunk && vad_state !== 'end') {
        console.log(`ğŸµ Skipping cache - no currentChunk (isCaching: ${context.isCaching}, vad_state: ${vad_state})`);
      } else if (!context.isCaching && vad_state !== 'end' && vad_state !== 'start') {
        console.log(`ğŸµ Skipping cache - not caching (isCaching: ${context.isCaching}, vad_state: ${vad_state})`);
      }
      
      if (currentChunk) {
        context.previousChunkBuffer.append(currentChunk);
      }
      context.globalTimeMs += CHUNK_DURATION_MS;
      
      // å¤„ç† cache_asr_trigger äº‹ä»¶ï¼ˆprefetchæ¨¡å¼ï¼‰
      if (vad_state === 'cache_asr_trigger' && context.isCaching) {
        console.log(`ğŸµ VAD CACHE_ASR_TRIGGER event received - speechCacheLength: ${context.speechAudioCache.length}, currentChunkSize: ${currentChunk?.length || 0}`);
        
        const speechEndTimeMs = context.globalTimeMs + (vad_offset_ms || 0);
        
        // åˆ›å»ºå½“å‰ç¼“å­˜çš„å‰¯æœ¬è¿›è¡Œprefetch ASRå¤„ç†
        const cachedDataForPrefetch = [...context.speechAudioCache];
        
        // æ·»åŠ å½“å‰chunkçš„triggeréƒ¨åˆ†
        if (currentChunk && vad_offset_ms && vad_offset_ms > 0) {
          const endByte = Math.min(currentChunk.length, vad_offset_ms * BYTES_PER_MS);
          const triggerChunk = currentChunk.slice(0, endByte);
          cachedDataForPrefetch.push(triggerChunk);
          console.log(`ğŸµ Added trigger chunk for prefetch: ${triggerChunk.length} bytes (offset: ${vad_offset_ms}ms)`);
        } else if (currentChunk) {
          cachedDataForPrefetch.push(currentChunk);
          console.log(`ğŸµ Added full current chunk for prefetch: ${currentChunk.length} bytes`);
        }
        
        // ç»Ÿè®¡prefetchæ•°æ®
        const totalBytes = cachedDataForPrefetch.reduce((sum, chunk) => sum + chunk.length, 0);
        const totalDurationMs = Math.round(totalBytes / BYTES_PER_MS);
        console.log(`ğŸµ Prefetch cache: ${cachedDataForPrefetch.length} chunks, ${totalBytes} bytes, ~${totalDurationMs}ms duration`);
        
        // å‘é€prefetch ASRè¯·æ±‚ï¼ˆæ ‡è®°ä¸ºé¢„å–ï¼‰
        if (cachedDataForPrefetch.length > 0) {
          const asrProvider = env.USE_FIREWORKS ? 'Fireworks' : 'Groq';
          console.log(`ğŸµ Starting PREFETCH ASR with ${asrProvider} (${cachedDataForPrefetch.length} chunks)`);
          
          // ä½¿ç”¨ç°æœ‰çš„ASRå‡½æ•°ï¼Œä¼ é€’prefetchæ ‡è®°
          const asrFunction = env.USE_FIREWORKS ? processAudioWithFireworks : processAudioWithGroq;
          
          asrFunction(cachedDataForPrefetch, websocket, context.speechStartTimeMs, speechEndTimeMs, true).catch(err => {
            console.error(`âŒ Prefetch ASR processing failed:`, err);
            websocket.send(JSON.stringify({
              type: 'transcription_error',
              error: `Failed to process prefetch audio with ${asrProvider} API.`,
              details: err.message,
              is_prefetch: true,
              timestamp: new Date().toISOString()
            }));
          });
        } else {
          console.log(`âš ï¸ No data for prefetch ASR - cachedDataForPrefetch is empty`);
        }
        
        // ç»§ç»­ç¼“å­˜ï¼Œä¸æ¸…ç©ºspeechAudioCacheï¼ˆä¸VAD endä¸åŒï¼‰
      }
      
      // è¯¦ç»†è°ƒè¯•VAD endäº‹ä»¶å¤„ç†
      if (vad_state === 'end') {
        console.log(`ğŸµ VAD END event received - isCaching: ${context.isCaching}, speechCacheLength: ${context.speechAudioCache.length}, currentChunkSize: ${currentChunk?.length || 0}`);
        
        if (context.isCaching) {
          const speechEndTimeMs = context.globalTimeMs + (vad_offset_ms || 0);
          
          // å¤„ç†å½“å‰chunkçš„endéƒ¨åˆ†
          let endChunkAdded = false;
          if (currentChunk && vad_offset_ms && vad_offset_ms > 0) {
            const endByte = Math.min(currentChunk.length, vad_offset_ms * BYTES_PER_MS);
            const endChunk = currentChunk.slice(0, endByte);
            context.speechAudioCache.push(endChunk);
            endChunkAdded = true;
            console.log(`ğŸµ Added VAD end chunk: ${endChunk.length} bytes (offset: ${vad_offset_ms}ms, endByte: ${endByte})`);
          } else if (currentChunk) {
            context.speechAudioCache.push(currentChunk);
            endChunkAdded = true;
            console.log(`ğŸµ Added full current chunk: ${currentChunk.length} bytes (no offset)`);
          } else {
            console.log(`ğŸµ No current chunk to add (currentChunk: ${currentChunk}, vad_offset_ms: ${vad_offset_ms})`);
          }
          
          context.isCaching = false;
          const cachedData = [...context.speechAudioCache];
          context.speechAudioCache = [];
          
          // è¯¦ç»†ç»Ÿè®¡ç¼“å­˜æ•°æ®
          const totalBytes = cachedData.reduce((sum, chunk) => sum + chunk.length, 0);
          const totalDurationMs = Math.round(totalBytes / BYTES_PER_MS);
          console.log(`ğŸµ Speech audio cache: ${cachedData.length} chunks, ${totalBytes} bytes, ~${totalDurationMs}ms duration`);
          
          const vadEndTimestamp = new Date().toISOString();
          websocket.send(context.MESSAGE_TEMPLATES.vad_cache_end_prefix + vadEndTimestamp + '"}');
          
          // æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®è¿›è¡ŒASR
          if (cachedData.length === 0) {
            console.error(`âŒ No audio data cached for ASR! isCaching was true but cache is empty.`);
            websocket.send(JSON.stringify({
              type: 'transcription_error',
              error: 'No audio data cached for transcription',
              details: 'speechAudioCache was empty despite isCaching being true',
              timestamp: new Date().toISOString()
            }));
          } else {
            const asrProvider = env.USE_FIREWORKS ? 'Fireworks' : 'Groq';
            const asrFunction = env.USE_FIREWORKS ? processAudioWithFireworks : processAudioWithGroq;
            console.log(`ğŸµ Starting ASR with ${asrProvider} (${cachedData.length} chunks, ${totalBytes} bytes)`);
            
            asrFunction(cachedData, websocket, context.speechStartTimeMs, speechEndTimeMs).catch(err => {
              console.error(`âŒ ASR processing failed:`, err);
              websocket.send(JSON.stringify({
                type: 'transcription_error',
                error: `Failed to process audio with ${asrProvider} API.`,
                details: err.message,
                timestamp: new Date().toISOString()
              }));
            });
          }
        } else {
          console.log(`ğŸµ VAD END ignored - not caching (isCaching: ${context.isCaching})`);
        }
      }
      break;

    case 'audio_stream_end':
      const endTimestamp = new Date().toISOString();
      websocket.send(context.MESSAGE_TEMPLATES.stream_end_ack_prefix + context.audioChunkCounter + ',"timestamp":"' + endTimestamp + '"}');
      break;

    default:
      const unknownType = parsedMessage.type || 'undefined';
      const errorResponse = JSON.stringify({
        error: "Unknown message type received",
        unknownType: unknownType,
        receivedMessage: parsedMessage,
        timestamp: new Date().toISOString()
      });
      websocket.send(errorResponse);
  }
}