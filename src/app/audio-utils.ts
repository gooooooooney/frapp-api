// Shared audio processing and WebSocket utilities
import { optimizedBase64Decode, RingBuffer, createOptimizedWavBlob } from './wav_utils';
import { validateAndConsumeTicket } from '../store/kv';
import { OptimizedAudioStorage, createAudioSession, AudioChunk } from './audio-storage';

// WebSocket message types
export interface AudioStreamStartMessage {
  type: 'audio_stream_start';
}

export interface AudioChunkMessage {
  type: 'audio_chunk';
  data: string;
  vad_state?: 'start' | 'end' | 'cache_asr_trigger' | 'cache_asr_drop';
  vad_offset_ms?: number;
  asr_prompt?: string;
}

export interface AudioStreamEndMessage {
  type: 'audio_stream_end';
}

export type WebSocketMessage = AudioStreamStartMessage | AudioChunkMessage | AudioStreamEndMessage;

// Response message types
export interface TranscriptionResult {
  type: 'transcription_result';
  text: string;
  speechStartTimeMs: number;
  speechEndTimeMs: number;
  timestamp: string;
  is_prefetch?: boolean;
  performance: {
    total_processing_ms: number;
    wav_creation_ms: number;
    api_fetch_ms: number;
    worker_timestamp: string;
    provider: string;
  };
}

export interface TranscriptionError {
  type: 'transcription_error';
  error: string;
  details?: any;
  timestamp: string;
}

// Audio configuration constants
export const CHUNK_DURATION_MS = 128;
export const BYTES_PER_MS = 32; // 16kHz, 16bit, mono = 32000 bytes/sec = 32 bytes/ms

// Global variables for environment
let envVariables: {
  GROQ_API_KEY: string;
  FIREWORKS_API_KEY: string;
  OPENROUTER_API_KEY: string;
  GEMINI_API_KEY: string;
  USE_FIREWORKS: boolean;
  DEBUG_MODE: boolean;
} | null = null;

export function initializeAudioEnv(env: Env): void {
  envVariables = {
    GROQ_API_KEY: env.GROQ_API_KEY,
    FIREWORKS_API_KEY: env.FIREWORKS_API_KEY,
    OPENROUTER_API_KEY: env.OPENROUTER_API_KEY,
    GEMINI_API_KEY: env.GEMINI_API_KEY,
    USE_FIREWORKS: env.USE_FIREWORKS === 'true',
    DEBUG_MODE: env.DEBUG_MODE === 'true',
  };
}

export function getAudioEnv() {
  if (!envVariables) {
    throw new Error('Audio environment not initialized');
  }
  return envVariables;
}

// User context interface for authenticated sessions
export interface AudioSessionContext {
  userId?: string;
  [key: string]: any;
}

// WebSocket session handler
export async function handleAudioSession(websocket: WebSocket, context?: AudioSessionContext): Promise<void> {
  websocket.accept();
  
  const env = getAudioEnv();
  const userId = context?.userId || 'anonymous';
  
  let audioChunkCounter = 0;
  let globalTimeMs = 0;
  let isCaching = false;
  let speechAudioCache: Uint8Array[] = [];
  let previousChunkBuffer = new RingBuffer(256 * BYTES_PER_MS); // 256msÁéØÂΩ¢ÁºìÂÜ≤Âå∫
  let speechStartTimeMs = 0; // ËÆ∞ÂΩïËØ≠Èü≥ÂºÄÂßãÁöÑÁúüÂÆûÊó∂Èó¥
  
  // È¢ÑÊûÑÂª∫ÈùôÊÄÅÊ∂àÊÅØÊ®°ÊùøÔºåÈÅøÂÖçÈáçÂ§çJSON.stringify
  const MESSAGE_TEMPLATES = {
    vad_cache_start: '{"type":"vad_cache_start"}',
    vad_cache_end_prefix: '{"type":"vad_cache_end","timestamp":"',
    stream_start_ack_prefix: '{"type":"audio_stream_start_ack","timestamp":"',
    stream_end_ack_prefix: '{"type":"audio_stream_end_ack","receivedChunks":',
  };

  websocket.addEventListener("message", async (event: MessageEvent) => {
    const data = event.data as string;
    {
      // Try to parse as JSON for our custom messages
      try {
        const parsedMessage: WebSocketMessage = JSON.parse(data);
        
        // Handle different message types
        switch (parsedMessage.type) {
          case 'audio_stream_start':
            audioChunkCounter = 0;
            globalTimeMs = 0;
            isCaching = false;
            speechAudioCache = [];
            previousChunkBuffer.clear();
            speechStartTimeMs = 0;
            const startTimestamp = new Date().toISOString();
            console.log(`Audio stream started for user: ${userId}`);
            websocket.send(MESSAGE_TEMPLATES.stream_start_ack_prefix + startTimestamp + '","userId":"' + userId + '"}');
            break;

          // --- ‰∏ªË¶Å‰øÆÊîπÈÉ®ÂàÜÔºöÂÆûÁé∞ VAD ÁºìÂ≠òÈÄªËæë ---
          case 'audio_chunk':
            const { vad_state, vad_offset_ms } = parsedMessage; 
            audioChunkCounter++;
            
            // ‰ºòÂåñÁöÑÈü≥È¢ëÊï∞ÊçÆËß£Á†Å
            let currentChunk: Uint8Array | null = null;
            if (parsedMessage.data && parsedMessage.data.length > 0) {
                currentChunk = optimizedBase64Decode(parsedMessage.data);
            }
            
            // 1. Â§ÑÁêÜ VAD 'start' ‰ø°Âè∑
            if (vad_state === 'start') {
                isCaching = true;
                speechAudioCache = [];
                
                // ËÆ∞ÂΩïËØ≠Èü≥ÂºÄÂßãÁöÑÁúüÂÆûÊó∂Èó¥ÔºàËÄÉËôëoffsetÔºâ
                speechStartTimeMs = globalTimeMs + (vad_offset_ms || 0);
                
                // ‰ªéÁéØÂΩ¢ÁºìÂÜ≤Âå∫Ëé∑ÂèñÂâçÁºÄÊï∞ÊçÆ
                if (vad_offset_ms && vad_offset_ms < 0) {
                    const bufferData = previousChunkBuffer.getData();
                    if (bufferData.length > 0) {
                        const offsetBytes = Math.abs(vad_offset_ms) * BYTES_PER_MS;
                        const startByte = Math.max(0, bufferData.length - offsetBytes);
                        const prefixChunk = bufferData.slice(startByte);
                        speechAudioCache.push(prefixChunk);
                    }
                }
                
                websocket.send(MESSAGE_TEMPLATES.vad_cache_start);
            }

            // 2. Â¶ÇÊûúÊ≠£Âú®ÁºìÂ≠ò‰∏îÊúâÊï∞ÊçÆÔºåÂàôÂ∞ÜÈü≥È¢ëÊï∞ÊçÆÂ≠òÂÖ• cacheÔºàVAD endÊÉÖÂÜµÁâπÊÆäÂ§ÑÁêÜÔºâ
            if (isCaching && currentChunk && vad_state !== 'end') {
                speechAudioCache.push(currentChunk);
            }
            
            // ‰ΩøÁî®ÁéØÂΩ¢ÁºìÂÜ≤Âå∫Áª¥Êä§256msÊªëÂä®Á™óÂè£
            if (currentChunk) {
                previousChunkBuffer.append(currentChunk);
            }
            globalTimeMs += CHUNK_DURATION_MS;
            
            // 3. Â§ÑÁêÜ VAD 'end' ‰ø°Âè∑
            if (vad_state === 'end' && isCaching) {
                // ËÆ°ÁÆóËØ≠Èü≥ÁªìÊùüÁöÑÁúüÂÆûÊó∂Èó¥
                const speechEndTimeMs = globalTimeMs + (vad_offset_ms || 0);
                
                // ÂØπ‰∫éVAD endÔºåÈúÄË¶ÅÊà™ÂèñÂΩìÂâçchunkÁöÑÂâçÂçäÈÉ®ÂàÜÔºà‰ªéÂºÄÂ§¥Âà∞offset‰ΩçÁΩÆÔºâ
                if (currentChunk && vad_offset_ms && vad_offset_ms > 0) {
                    const endByte = Math.min(currentChunk.length, vad_offset_ms * BYTES_PER_MS);
                    const endChunk = currentChunk.slice(0, endByte);
                    speechAudioCache.push(endChunk);
                } else if (currentChunk) {
                    speechAudioCache.push(currentChunk);
                }
                
                isCaching = false;

                const cachedData = [...speechAudioCache];
                speechAudioCache = [];

                const vadEndTimestamp = new Date().toISOString();
                websocket.send(MESSAGE_TEMPLATES.vad_cache_end_prefix + vadEndTimestamp + '"}');
                
                // Ë∞ÉÁî®ASR APIÔºå‰º†ÈÄíÊó∂Èó¥‰ø°ÊÅØ
                const asrFunction = env.USE_FIREWORKS ? processAudioWithFireworks : processAudioWithGroq;
                asrFunction(cachedData, websocket, speechStartTimeMs, speechEndTimeMs).catch(err => {
                    websocket.send(JSON.stringify({
                        type: 'transcription_error',
                        error: `Failed to process audio with ${env.USE_FIREWORKS ? 'Fireworks' : 'Groq'} API.`,
                        details: err.message,
                        timestamp: new Date().toISOString()
                    }));
                });
            }
            break;
          // --- ‰øÆÊîπÁªìÊùü ---

          case 'audio_stream_end':
            const endTimestamp = new Date().toISOString();
            websocket.send(MESSAGE_TEMPLATES.stream_end_ack_prefix + audioChunkCounter + ',"timestamp":"' + endTimestamp + '"}');
            break;
          default:
            // Unknown JSON message - send back a specific error message with the unknown message type
            const unknownMessage = parsedMessage as any;
            const unknownType = unknownMessage.type || 'undefined';
            const errorResponse = JSON.stringify({
              error: "Unknown message type received",
              unknownType: unknownType,
              receivedMessage: parsedMessage,
              timestamp: new Date().toISOString()
            });
            websocket.send(errorResponse);
        }
      } catch (parseError) {
        // Not JSON - send back a specific error message with parse details
        const err = parseError as Error;
        const errorResponse = JSON.stringify({
          error: "Failed to parse message as JSON",
          parseError: err.message,
          receivedData: typeof data === 'string' ? data.substring(0, 100) + (data.length > 100 ? '...' : '') : 'Non-string data',
          timestamp: new Date().toISOString()
        });
        websocket.send(errorResponse);
      }
    }
  })

  websocket.addEventListener("close", () => {
    websocket.close();
    console.log(`WebSocket connection closed for user: ${userId}`);
  })
}

export async function processAudioWithFireworks(audioDataChunks: Uint8Array[], websocket: WebSocket, speechStartTimeMs: number, speechEndTimeMs: number, isPrefetch: boolean = false): Promise<void> {
    const env = getAudioEnv();
    const startTime = Date.now();
    
    console.log(`üî• Fireworks ASR called with ${audioDataChunks.length} chunks, speechTime: ${speechStartTimeMs}ms - ${speechEndTimeMs}ms`);
    
    if (!env.FIREWORKS_API_KEY) {
        console.error('‚ùå Fireworks ASR failed: FIREWORKS_API_KEY not set');
        throw new Error("FIREWORKS_API_KEY secret is not set in the worker environment.");
    }
    if (audioDataChunks.length === 0) {
        console.error('‚ùå Fireworks ASR aborted: empty audioDataChunks array');
        return;
    }

    // 1. ‰ΩøÁî®‰ºòÂåñÁöÑWAVÂàõÂª∫
    const wavStartTime = Date.now();
    const audioBlob = createOptimizedWavBlob(audioDataChunks);
    const wavEndTime = Date.now();
    
    // ÂèëÈÄÅÈü≥È¢ëÊï∞ÊçÆÂõûÂÆ¢Êà∑Á´Ø‰æõÊú¨Âú∞‰øùÂ≠òÊ£ÄÊü•
    if (env.DEBUG_MODE) {
      const audioArrayBuffer = await audioBlob.arrayBuffer();
      const audioBase64 = btoa(String.fromCharCode(...new Uint8Array(audioArrayBuffer)));
      const debugTimestamp = new Date().toISOString();
      // ‰ΩøÁî®Â≠óÁ¨¶‰∏≤ÊãºÊé•ÈÅøÂÖçJSON.stringify
      websocket.send('{"type":"debug_audio","audioData":"' + audioBase64 + 
                     '","speechStartTimeMs":' + speechStartTimeMs + 
                     ',"speechEndTimeMs":' + speechEndTimeMs + 
                     ',"timestamp":"' + debugTimestamp + '"}');
    }

    // 2. ÂàõÂª∫ FormData
    const formData = new FormData();
    formData.append('file', audioBlob, 'audio.wav');
    formData.append('model', 'whisper-v3-turbo');
    formData.append('temperature', '0');

    // 3. ÂèëËµ∑ fetch ËØ∑Ê±ÇÂà∞ Fireworks API
    const apiStartTime = Date.now();
    const response = await fetch("https://audio-turbo.us-virginia-1.direct.fireworks.ai/v1/audio/transcriptions", {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${env.FIREWORKS_API_KEY}`,
        },
        body: formData,
    });

    // 4. Â§ÑÁêÜÂìçÂ∫î
    const result = await response.json() as any;
    const apiEndTime = Date.now();
    const totalEndTime = Date.now();

    if (!response.ok) {
        console.error("Fireworks API returned an error:", result);
        throw new Error(`Fireworks API error: ${response.status} ${response.statusText} - ${JSON.stringify(result)}`);
    }

    // 5. ÂÅ•Â£ÆÂú∞Ê£ÄÊü•Âπ∂ÊèêÂèñ text Â≠óÊÆµ
    if (result && result.text !== undefined) {
        const transcriptionResult: TranscriptionResult = {
            type: 'transcription_result',
            text: result.text,
            speechStartTimeMs: speechStartTimeMs,
            speechEndTimeMs: speechEndTimeMs,
            timestamp: new Date().toISOString(),
            is_prefetch: isPrefetch,
            performance: {
                total_processing_ms: totalEndTime - startTime,
                wav_creation_ms: wavEndTime - wavStartTime,
                api_fetch_ms: apiEndTime - apiStartTime,
                worker_timestamp: new Date().toISOString(),
                provider: 'fireworks'
            }
        };
        websocket.send(JSON.stringify(transcriptionResult));
    } else {
        const errorResult: TranscriptionError = {
            type: 'transcription_error',
            error: 'Invalid response format from Fireworks API.',
            details: result,
            timestamp: new Date().toISOString()
        };
        websocket.send(JSON.stringify(errorResult));
    }
}

export async function processAudioWithGroq(audioDataChunks: Uint8Array[], websocket: WebSocket, speechStartTimeMs: number, speechEndTimeMs: number, isPrefetch: boolean = false): Promise<void> {
    const env = getAudioEnv();
    const startTime = Date.now();
    
    console.log(`ü§ñ Groq ASR called with ${audioDataChunks.length} chunks, speechTime: ${speechStartTimeMs}ms - ${speechEndTimeMs}ms`);
    
    if (!env.GROQ_API_KEY) {
        console.error('‚ùå Groq ASR failed: GROQ_API_KEY not set');
        throw new Error("GROQ_API_KEY secret is not set in the worker environment.");
    }
    if (audioDataChunks.length === 0) {
        console.error('‚ùå Groq ASR aborted: empty audioDataChunks array');
        return;
    }

    // 1. ‰ΩøÁî®‰ºòÂåñÁöÑWAVÂàõÂª∫
    const wavStartTime = Date.now();
    const audioBlob = createOptimizedWavBlob(audioDataChunks);
    const wavEndTime = Date.now();
    
    // ÂèëÈÄÅÈü≥È¢ëÊï∞ÊçÆÂõûÂÆ¢Êà∑Á´Ø‰æõÊú¨Âú∞‰øùÂ≠òÊ£ÄÊü•
    if (env.DEBUG_MODE) {
      const audioArrayBuffer = await audioBlob.arrayBuffer();
      const audioBase64 = btoa(String.fromCharCode(...new Uint8Array(audioArrayBuffer)));
      const debugTimestamp = new Date().toISOString();
      // ‰ΩøÁî®Â≠óÁ¨¶‰∏≤ÊãºÊé•ÈÅøÂÖçJSON.stringify
      websocket.send('{"type":"debug_audio","audioData":"' + audioBase64 + 
                     '","speechStartTimeMs":' + speechStartTimeMs + 
                     ',"speechEndTimeMs":' + speechEndTimeMs + 
                     ',"timestamp":"' + debugTimestamp + '"}');
    }

    // 2. ÂàõÂª∫ FormData
    const formData = new FormData();
    formData.append('file', audioBlob, 'audio.wav');
    formData.append('model', 'whisper-large-v3-turbo');
    formData.append('response_format', 'verbose_json'); 

    // 3. ÂèëËµ∑ fetch ËØ∑Ê±ÇÂà∞ Groq API
    const groqStartTime = Date.now();
    const response = await fetch("https://api.groq.com/openai/v1/audio/transcriptions", {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${env.GROQ_API_KEY}`,
        },
        body: formData,
    });

    // 4. Â§ÑÁêÜÂìçÂ∫î
    const result = await response.json() as any;
    const groqEndTime = Date.now();
    const totalEndTime = Date.now();

    if (!response.ok) {
        // Â¶ÇÊûú HTTP Áä∂ÊÄÅÁ†Å‰∏çÊòØ 2xxÔºåÂàôÊäõÂá∫ÈîôËØØ
        console.error("Groq API returned an error:", result);
        throw new Error(`Groq API error: ${response.status} ${response.statusText} - ${JSON.stringify(result)}`);
    }

    // 5. ÂÅ•Â£ÆÂú∞Ê£ÄÊü•Âπ∂ÊèêÂèñ text Â≠óÊÆµ
    if (result && result.text !== undefined) {
        const transcriptionResult: TranscriptionResult = {
            type: 'transcription_result',
            text: result.text,
            speechStartTimeMs: speechStartTimeMs,
            speechEndTimeMs: speechEndTimeMs,
            timestamp: new Date().toISOString(),
            is_prefetch: isPrefetch,
            performance: {
                total_processing_ms: totalEndTime - startTime,
                wav_creation_ms: wavEndTime - wavStartTime,
                api_fetch_ms: groqEndTime - groqStartTime,
                worker_timestamp: new Date().toISOString(),
                provider: 'groq'
            }
        };
        websocket.send(JSON.stringify(transcriptionResult));
    } else {
        // Â§±Ë¥•ÔºöËøîÂõûÁöÑ JSON ‰∏≠Ê≤°Êúâ text Â≠óÊÆµÔºåÂΩì‰ΩúÈîôËØØÂ§ÑÁêÜ
        const errorResult: TranscriptionError = {
            type: 'transcription_error',
            error: 'Invalid response format from Groq API.',
            details: result,
            timestamp: new Date().toISOString()
        };
        websocket.send(JSON.stringify(errorResult));
    }
}

// Authentication message types for first-message auth
export interface AuthMessage {
  type: 'auth';
  ticket: string;
}

export interface AuthSuccessMessage {
  type: 'auth_success';
  userId: string;
  timestamp: string;
}

export interface AuthErrorMessage {
  type: 'auth_error';
  error: string;
  timestamp: string;
}

// Enhanced WebSocket session handler with First Message Authentication
export async function handleSecureAudioSession(websocket: WebSocket, env: any): Promise<void> {
  console.log('üîê Accepting WebSocket connection...');
  websocket.accept();
  console.log('‚úÖ WebSocket accepted successfully');
  
  let isAuthenticated = false;
  let userId: string | null = null;
  let audioStorage: OptimizedAudioStorage | null = null;
  const connectionStart = Date.now();

  // Add error and close handlers for debugging
  websocket.addEventListener('error', (event) => {
    console.error('‚ùå WebSocket error in handleSecureAudioSession:', event);
  });
  
  websocket.addEventListener('close', (event) => {
    console.log('üîå WebSocket closed in handleSecureAudioSession:', event.code, event.reason);
    if (audioStorage) {
      console.log('üßπ Cleaning up audio storage...');
      audioStorage.cleanup().catch(err => console.error('‚ùå Audio storage cleanup failed:', err));
    }
  });
  
  websocket.addEventListener('open', () => {
    console.log('‚úÖ WebSocket opened in handleSecureAudioSession');
  });

  // Authentication timeout (5 seconds)
  const authTimeout = setTimeout(() => {
    if (!isAuthenticated) {
      console.log('‚ùå WebSocket connection timed out - no authentication within 5 seconds');
      websocket.send(JSON.stringify({
        type: 'auth_error',
        error: 'Authentication timeout - connection closed',
        timestamp: new Date().toISOString()
      }));
      websocket.close(1008, 'Authentication timeout');
    }
  }, 5000);

  let audioChunkCounter = 0;
  let globalTimeMs = 0;
  let isCaching = false;
  let speechAudioCache: Uint8Array[] = [];
  let previousChunkBuffer = new RingBuffer(256 * BYTES_PER_MS);
  let speechStartTimeMs = 0;
  
  const MESSAGE_TEMPLATES = {
    vad_cache_start: '{"type":"vad_cache_start"}',
    vad_cache_end_prefix: '{"type":"vad_cache_end","timestamp":"',
    stream_start_ack_prefix: '{"type":"audio_stream_start_ack","timestamp":"',
    stream_end_ack_prefix: '{"type":"audio_stream_end_ack","receivedChunks":',
  };

  websocket.addEventListener("message", async (event: MessageEvent) => {
    const data = event.data as string;
    
    try {
      const parsedMessage = JSON.parse(data);
      
      // Handle authentication first
      if (!isAuthenticated) {
        if (parsedMessage.type === 'auth') {
          const authMessage = parsedMessage as AuthMessage;
          
          if (!authMessage.ticket) {
            websocket.send(JSON.stringify({
              type: 'auth_error',
              error: 'Missing ticket in authentication message',
              timestamp: new Date().toISOString()
            }));
            websocket.close(1008, 'Invalid authentication');
            return;
          }

          // Validate ticket
          const validation = await validateAndConsumeTicket(authMessage.ticket, env.WS_TICKETS_KV);
          
          if (!validation) {
            websocket.send(JSON.stringify({
              type: 'auth_error',
              error: 'Invalid or expired ticket',
              timestamp: new Date().toISOString()
            }));
            websocket.close(1008, 'Authentication failed');
            return;
          }

          // Authentication successful
          isAuthenticated = true;
          userId = validation.userId;
          clearTimeout(authTimeout);
          
          // ÂàùÂßãÂåñÈü≥È¢ëÂ≠òÂÇ® (‰ºòÂåñÁöÑÂÜÖÂ≠òÁºìÂ≠òÊñπÊ°à)
          try {
            audioStorage = createAudioSession(userId, env.FRAPP_FILES_STORE, {
              windowSizeMs: 2 * 60 * 1000,      // 2ÂàÜÈíüÁ™óÂè£
              uploadIntervalMs: 60 * 1000,      // 1ÂàÜÈíü‰∏ä‰º†
              maxMemoryMB: 10,                  // 10MBÈôêÂà∂
              enableDebug: env.DEBUG_MODE === 'true',
              storeOriginalAudio: true,         // Â≠òÂÇ®ÂÆåÊï¥ÂéüÂßãÈü≥È¢ëÊµÅ
              storeVadSegments: false           // ‰∏çÂçïÁã¨Â≠òÂÇ®VADÁâáÊÆµ
            });
            console.log(`üéµ Audio storage initialized for user: ${userId}`);
          } catch (error) {
            console.error('‚ùå Failed to initialize audio storage:', error);
            // ÈùûËá¥ÂëΩÈîôËØØÔºåÁªßÁª≠ËøêË°å‰ΩÜ‰∏çÂ≠òÂÇ®Èü≥È¢ë
          }
          
          console.log(`WebSocket authenticated for user: ${userId} (took ${Date.now() - connectionStart}ms)`);
          
          websocket.send(JSON.stringify({
            type: 'auth_success',
            userId: userId,
            timestamp: new Date().toISOString()
          }));
          
          return;
        } else {
          // Not authenticated and not an auth message
          websocket.send(JSON.stringify({
            type: 'auth_error',
            error: 'Must authenticate first with auth message',
            timestamp: new Date().toISOString()
          }));
          websocket.close(1008, 'Authentication required');
          return;
        }
      }

      // Handle audio messages (only after authentication)
      await handleAudioMessage(parsedMessage, websocket, {
        audioChunkCounter,
        globalTimeMs,
        isCaching,
        speechAudioCache,
        previousChunkBuffer,
        speechStartTimeMs,
        MESSAGE_TEMPLATES,
        userId: userId!,
        audioStorage
      });

    } catch (parseError) {
      const err = parseError as Error;
      const errorResponse = JSON.stringify({
        error: "Failed to parse message as JSON",
        parseError: err.message,
        receivedData: typeof data === 'string' ? data.substring(0, 100) + (data.length > 100 ? '...' : '') : 'Non-string data',
        timestamp: new Date().toISOString()
      });
      websocket.send(errorResponse);
    }
  });

  websocket.addEventListener("close", async () => {
    clearTimeout(authTimeout);
    
    // Ê∏ÖÁêÜÈü≥È¢ëÂ≠òÂÇ®ËµÑÊ∫ê
    if (audioStorage) {
      try {
        await audioStorage.cleanup();
        console.log(`üßπ Audio storage cleaned up for user: ${userId}`);
      } catch (error) {
        console.error('‚ùå Error during audio storage cleanup:', error);
      }
    }
    
    const status = isAuthenticated ? `authenticated user: ${userId}` : 'unauthenticated connection';
    console.log(`WebSocket connection closed for ${status}`);
  });
}

// Extracted audio message handler for cleaner code
async function handleAudioMessage(
  parsedMessage: any,
  websocket: WebSocket,
  context: {
    audioChunkCounter: number;
    globalTimeMs: number;
    isCaching: boolean;
    speechAudioCache: Uint8Array[];
    previousChunkBuffer: RingBuffer;
    speechStartTimeMs: number;
    MESSAGE_TEMPLATES: any;
    userId: string;
    audioStorage: OptimizedAudioStorage | null;
  }
) {
  const env = getAudioEnv();

  switch (parsedMessage.type) {
    case 'audio_stream_start':
      context.audioChunkCounter = 0;
      context.globalTimeMs = 0;
      context.isCaching = false;
      context.speechAudioCache = [];
      context.previousChunkBuffer.clear();
      context.speechStartTimeMs = 0;
      const startTimestamp = new Date().toISOString();
      console.log(`Audio stream started for user: ${context.userId}`);
      websocket.send(context.MESSAGE_TEMPLATES.stream_start_ack_prefix + startTimestamp + '","userId":"' + context.userId + '"}');
      break;

    case 'audio_chunk':
      const { vad_state, vad_offset_ms } = parsedMessage;
      context.audioChunkCounter++;
      
      let currentChunk: Uint8Array | null = null;
      if (parsedMessage.data && parsedMessage.data.length > 0) {
        try {
          currentChunk = optimizedBase64Decode(parsedMessage.data);
          console.log(`üéµ Decoded audio chunk: ${parsedMessage.data.length} base64 chars -> ${currentChunk?.length || 0} bytes (VAD: ${vad_state || 'none'})`);
        } catch (decodeError) {
          console.error('‚ùå Base64 decode failed:', decodeError);
          currentChunk = null;
        }
        
        // ÂéüÂßãÂÆåÊï¥Èü≥È¢ëÂ≠òÂÇ®Â§ÑÁêÜ (ÂºÇÊ≠•Ôºå‰∏çÈòªÂ°ûÂÆûÊó∂ËΩ¨ÂΩï)
        // Â≠òÂÇ®ÊâÄÊúâÈü≥È¢ëÂùóÔºå‰∏ç‰æùËµñVADÁä∂ÊÄÅÔºå‰øùÊåÅÂÆåÊï¥Èü≥È¢ëÊµÅËøûÁª≠ÊÄß
        if (currentChunk && context.audioStorage) {
          const audioChunk: AudioChunk = {
            timestamp: context.globalTimeMs,
            data: currentChunk,
            vadState: vad_state,
            vadOffset: vad_offset_ms
          };
          
          // ÂºÇÊ≠•Â§ÑÁêÜÔºå‰∏çÁ≠âÂæÖÂÆåÊàê
          context.audioStorage.processAudioChunk(audioChunk).catch(err => {
            console.error('‚ùå Original audio storage processing error:', err);
          });
        }
      } else {
        console.log(`üéµ No audio data in message (data length: ${parsedMessage.data?.length || 0})`);
      }
      
      if (vad_state === 'start') {
        console.log(`üéµ VAD START event received - globalTimeMs: ${context.globalTimeMs}, vad_offset_ms: ${vad_offset_ms}`);
        
        context.isCaching = true;
        context.speechAudioCache = [];
        context.speechStartTimeMs = context.globalTimeMs + (vad_offset_ms || 0);
        
        // Â§ÑÁêÜË¥üÂÅèÁßªÔºàÈúÄË¶Å‰ªéring bufferËé∑ÂèñÂâçÁºÄÊï∞ÊçÆÔºâ
        if (vad_offset_ms && vad_offset_ms < 0) {
          const bufferData = context.previousChunkBuffer.getData();
          console.log(`üéµ VAD START negative offset: ${vad_offset_ms}ms, ringBufferSize: ${bufferData.length} bytes`);
          
          if (bufferData.length > 0) {
            const offsetBytes = Math.abs(vad_offset_ms) * BYTES_PER_MS;
            const startByte = Math.max(0, bufferData.length - offsetBytes);
            const prefixChunk = bufferData.slice(startByte);
            context.speechAudioCache.push(prefixChunk);
            console.log(`üéµ Added prefix chunk: ${prefixChunk.length} bytes (startByte: ${startByte})`);
          }
        }
        
        console.log(`üéµ VAD START complete - speechStartTimeMs: ${context.speechStartTimeMs}, cacheLength: ${context.speechAudioCache.length}`);
        websocket.send(context.MESSAGE_TEMPLATES.vad_cache_start);
      }

      // ÁºìÂ≠ò‰∏≠Èó¥Èü≥È¢ëÂùóÔºàVAD startÂà∞end‰πãÈó¥Ôºâ
      if (context.isCaching && currentChunk && vad_state !== 'end') {
        context.speechAudioCache.push(currentChunk);
        console.log(`üéµ Cached audio chunk: ${currentChunk.length} bytes (total cache: ${context.speechAudioCache.length} chunks)`);
      } else if (context.isCaching && !currentChunk && vad_state !== 'end') {
        console.log(`üéµ Skipping cache - no currentChunk (isCaching: ${context.isCaching}, vad_state: ${vad_state})`);
      } else if (!context.isCaching && vad_state !== 'end' && vad_state !== 'start') {
        console.log(`üéµ Skipping cache - not caching (isCaching: ${context.isCaching}, vad_state: ${vad_state})`);
      }
      
      if (currentChunk) {
        context.previousChunkBuffer.append(currentChunk);
      }
      context.globalTimeMs += CHUNK_DURATION_MS;
      
      // Â§ÑÁêÜ cache_asr_trigger ‰∫ã‰ª∂ÔºàprefetchÊ®°ÂºèÔºâ
      if (vad_state === 'cache_asr_trigger' && context.isCaching) {
        console.log(`üéµ VAD CACHE_ASR_TRIGGER event received - speechCacheLength: ${context.speechAudioCache.length}, currentChunkSize: ${currentChunk?.length || 0}`);
        
        const speechEndTimeMs = context.globalTimeMs + (vad_offset_ms || 0);
        
        // ÂàõÂª∫ÂΩìÂâçÁºìÂ≠òÁöÑÂâØÊú¨ËøõË°åprefetch ASRÂ§ÑÁêÜ
        const cachedDataForPrefetch = [...context.speechAudioCache];
        
        // Ê∑ªÂä†ÂΩìÂâçchunkÁöÑtriggerÈÉ®ÂàÜ
        if (currentChunk && vad_offset_ms && vad_offset_ms > 0) {
          const endByte = Math.min(currentChunk.length, vad_offset_ms * BYTES_PER_MS);
          const triggerChunk = currentChunk.slice(0, endByte);
          cachedDataForPrefetch.push(triggerChunk);
          console.log(`üéµ Added trigger chunk for prefetch: ${triggerChunk.length} bytes (offset: ${vad_offset_ms}ms)`);
        } else if (currentChunk) {
          cachedDataForPrefetch.push(currentChunk);
          console.log(`üéµ Added full current chunk for prefetch: ${currentChunk.length} bytes`);
        }
        
        // ÁªüËÆ°prefetchÊï∞ÊçÆ
        const totalBytes = cachedDataForPrefetch.reduce((sum, chunk) => sum + chunk.length, 0);
        const totalDurationMs = Math.round(totalBytes / BYTES_PER_MS);
        console.log(`üéµ Prefetch cache: ${cachedDataForPrefetch.length} chunks, ${totalBytes} bytes, ~${totalDurationMs}ms duration`);
        
        // ÂèëÈÄÅprefetch ASRËØ∑Ê±ÇÔºàÊ†áËÆ∞‰∏∫È¢ÑÂèñÔºâ
        if (cachedDataForPrefetch.length > 0) {
          const asrProvider = env.USE_FIREWORKS ? 'Fireworks' : 'Groq';
          console.log(`üéµ Starting PREFETCH ASR with ${asrProvider} (${cachedDataForPrefetch.length} chunks)`);
          
          // ‰ΩøÁî®Áé∞ÊúâÁöÑASRÂáΩÊï∞Ôºå‰º†ÈÄíprefetchÊ†áËÆ∞
          const asrFunction = env.USE_FIREWORKS ? processAudioWithFireworks : processAudioWithGroq;
          
          asrFunction(cachedDataForPrefetch, websocket, context.speechStartTimeMs, speechEndTimeMs, true).catch(err => {
            console.error(`‚ùå Prefetch ASR processing failed:`, err);
            websocket.send(JSON.stringify({
              type: 'transcription_error',
              error: `Failed to process prefetch audio with ${asrProvider} API.`,
              details: err.message,
              is_prefetch: true,
              timestamp: new Date().toISOString()
            }));
          });
        } else {
          console.log(`‚ö†Ô∏è No data for prefetch ASR - cachedDataForPrefetch is empty`);
        }
        
        // ÁªßÁª≠ÁºìÂ≠òÔºå‰∏çÊ∏ÖÁ©∫speechAudioCacheÔºà‰∏éVAD end‰∏çÂêåÔºâ
      }
      
      // ËØ¶ÁªÜË∞ÉËØïVAD end‰∫ã‰ª∂Â§ÑÁêÜ
      if (vad_state === 'end') {
        console.log(`üéµ VAD END event received - isCaching: ${context.isCaching}, speechCacheLength: ${context.speechAudioCache.length}, currentChunkSize: ${currentChunk?.length || 0}`);
        
        if (context.isCaching) {
          const speechEndTimeMs = context.globalTimeMs + (vad_offset_ms || 0);
          
          // Â§ÑÁêÜÂΩìÂâçchunkÁöÑendÈÉ®ÂàÜ
          let endChunkAdded = false;
          if (currentChunk && vad_offset_ms && vad_offset_ms > 0) {
            const endByte = Math.min(currentChunk.length, vad_offset_ms * BYTES_PER_MS);
            const endChunk = currentChunk.slice(0, endByte);
            context.speechAudioCache.push(endChunk);
            endChunkAdded = true;
            console.log(`üéµ Added VAD end chunk: ${endChunk.length} bytes (offset: ${vad_offset_ms}ms, endByte: ${endByte})`);
          } else if (currentChunk) {
            context.speechAudioCache.push(currentChunk);
            endChunkAdded = true;
            console.log(`üéµ Added full current chunk: ${currentChunk.length} bytes (no offset)`);
          } else {
            console.log(`üéµ No current chunk to add (currentChunk: ${currentChunk}, vad_offset_ms: ${vad_offset_ms})`);
          }
          
          context.isCaching = false;
          const cachedData = [...context.speechAudioCache];
          context.speechAudioCache = [];
          
          // ËØ¶ÁªÜÁªüËÆ°ÁºìÂ≠òÊï∞ÊçÆ
          const totalBytes = cachedData.reduce((sum, chunk) => sum + chunk.length, 0);
          const totalDurationMs = Math.round(totalBytes / BYTES_PER_MS);
          console.log(`üéµ Speech audio cache: ${cachedData.length} chunks, ${totalBytes} bytes, ~${totalDurationMs}ms duration`);
          
          const vadEndTimestamp = new Date().toISOString();
          websocket.send(context.MESSAGE_TEMPLATES.vad_cache_end_prefix + vadEndTimestamp + '"}');
          
          // Ê£ÄÊü•ÊòØÂê¶ÊúâÊï∞ÊçÆËøõË°åASR
          if (cachedData.length === 0) {
            console.error(`‚ùå No audio data cached for ASR! isCaching was true but cache is empty.`);
            websocket.send(JSON.stringify({
              type: 'transcription_error',
              error: 'No audio data cached for transcription',
              details: 'speechAudioCache was empty despite isCaching being true',
              timestamp: new Date().toISOString()
            }));
          } else {
            const asrProvider = env.USE_FIREWORKS ? 'Fireworks' : 'Groq';
            const asrFunction = env.USE_FIREWORKS ? processAudioWithFireworks : processAudioWithGroq;
            console.log(`üéµ Starting ASR with ${asrProvider} (${cachedData.length} chunks, ${totalBytes} bytes)`);
            
            asrFunction(cachedData, websocket, context.speechStartTimeMs, speechEndTimeMs).catch(err => {
              console.error(`‚ùå ASR processing failed:`, err);
              websocket.send(JSON.stringify({
                type: 'transcription_error',
                error: `Failed to process audio with ${asrProvider} API.`,
                details: err.message,
                timestamp: new Date().toISOString()
              }));
            });
          }
        } else {
          console.log(`üéµ VAD END ignored - not caching (isCaching: ${context.isCaching})`);
        }
      }
      break;

    case 'audio_stream_end':
      const endTimestamp = new Date().toISOString();
      websocket.send(context.MESSAGE_TEMPLATES.stream_end_ack_prefix + context.audioChunkCounter + ',"timestamp":"' + endTimestamp + '"}');
      break;

    default:
      const unknownType = parsedMessage.type || 'undefined';
      const errorResponse = JSON.stringify({
        error: "Unknown message type received",
        unknownType: unknownType,
        receivedMessage: parsedMessage,
        timestamp: new Date().toISOString()
      });
      websocket.send(errorResponse);
  }
}